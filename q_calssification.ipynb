{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "import gc\n",
    "class FreeMemory(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_freq=None):\n",
    "        super().__init__()\n",
    "        self.log_freq = log_freq\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.log_freq and epoch % self.log_freq == 0:\n",
    "            print(f'epoch {epoch} ended, info: {logs}')\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POINTS_NUM = 10\n",
    "CLASSES_NUM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "points = np.load(f'/host/dissertation/proccessed_data/points_{POINTS_NUM}.npy')\n",
    "q = np.load('/host/dissertation/proccessed_data/q.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q[q == -1] = 0\n",
    "q_labels = tf.one_hot(q, CLASSES_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_dataframe(fit_history):\n",
    "    df = pd.DataFrame(fit_history.history)\n",
    "    df.insert(0, 'epoch', fit_history.epoch)\n",
    "    return df\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "def plot_loss_history(fit_history, metric = 'loss', title = \"\", threshold_multiplier = 10):\n",
    "\n",
    "    if isinstance(fit_history, pd.core.frame.DataFrame):\n",
    "        df = fit_history\n",
    "    else:\n",
    "        df = make_dataframe(fit_history)\n",
    "\n",
    "    train_1st_percentile = np.percentile(df[metric].values, 1)\n",
    "    val_1st_percentile = np.percentile(df[f'val_{metric}'].values, 1)\n",
    "    print(f'1st percentile of train {metric}:       {train_1st_percentile:.4e}')\n",
    "    print(f'1st percentile of validation {metric}:  {val_1st_percentile:.4e}')\n",
    "    \n",
    "    threshold = np.maximum(train_1st_percentile, val_1st_percentile)\n",
    "    fig, axes = plt.subplots(3, sharex=True, figsize=(8,6))\n",
    "    axes[0].set_title(f\"{title} {metric} history\")\n",
    "    for axis in axes:\n",
    "        axis.set_ylim((0, threshold_multiplier*threshold))\n",
    "        axis.plot(df['epoch'], df[metric], label='training set')\n",
    "        axis.plot(df['epoch'], df[f'val_{metric}'], linestyle='dashed', label='validation set')\n",
    "        threshold_multiplier /= 2\n",
    "    axes[-1].legend(loc=\"lower left\")\n",
    "    axes[-1].set_xlabel(\"epoch no.\")\n",
    "    fig.supylabel(f\"{metric} value\")\n",
    "\n",
    "def plot_accuracy_history(fit_history, moving_average_window = (10,), metric = 'accuracy'):\n",
    "    \n",
    "    if isinstance(fit_history, pd.core.frame.DataFrame):\n",
    "        df = fit_history\n",
    "    else:\n",
    "        df = make_dataframe(fit_history)\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(df['epoch'], df[metric], label=f'training set {metric}')\n",
    "    plt.plot(df['epoch'], df[f'val_{metric}'], linestyle='dotted', linewidth=0.5, label=f'validation set {metric}')\n",
    "    for w in moving_average_window:\n",
    "        plt.plot(df['epoch'][w-1:], moving_average(df[f'val_{metric}'], w), linestyle='dashed', label=f'validation set {metric} moving average, w={w}')\n",
    "    \n",
    "    plt.xlabel('epoch no.')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvnn.layers as complex_layers\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(complex_layers.ComplexInput(input_shape=(POINTS_NUM,)))\n",
    "model.add(complex_layers.ComplexDense(units=64, activation='cart_relu'))\n",
    "model.add(complex_layers.ComplexDense(units=CLASSES_NUM, activation='sigmoid_real'))\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(clipnorm=1.), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(points, q_labels, epochs=250, validation_split=0.2, verbose=0, callbacks=[FreeMemory(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss_history(history)\n",
    "plot_accuracy_history(history, (40,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataframe(history).to_csv('/host/dissertation/trained_models/history.csv', index=False)\n",
    "df = pd.read_csv('/host/dissertation/trained_models/history.csv')\n",
    "\n",
    "tail_size = 20\n",
    "val_accuracy_tail = df['val_accuracy'][-tail_size:]\n",
    "accuracy_tail = df['accuracy'][-tail_size:]\n",
    "print('-------------')\n",
    "print('val_accuracy')\n",
    "print(f'   avg: {np.average(val_accuracy_tail):.6f}')\n",
    "print(f'   med: {np.median(val_accuracy_tail):.6f}')\n",
    "print(f'   std: {np.std(val_accuracy_tail):.6f}')\n",
    "print('accuracy')\n",
    "print(f'   max: {np.max(accuracy_tail):.6f}')\n",
    "print(f'   min: {np.min(accuracy_tail):.6f}')\n",
    "print('-------------\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### space search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAIL_SIZE = 20\n",
    "EPOCHS_NUM = 30 # 250\n",
    "hiddenlayers_number_space = [1,2,3]\n",
    "layer_neurons_number_space = [32,64,96]\n",
    "layer_activation_func_space = ['pol_tanh','pol_sigmoid','pol_selu','cart_sigmoid','cart_relu','cart_tanh'] #['cart_relu']#\n",
    "\n",
    "import itertools\n",
    "hp_list = list(itertools.product(hiddenlayers_number_space,layer_neurons_number_space,layer_activation_func_space))\n",
    "param = {'hidden_layers': 0, 'neurons_num': 1, 'activation_func': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "\n",
    "TIMESTAMP = int(time.time())\n",
    "WORKING_DIRECTORY = f'/host/dissertation/trained_models/{TIMESTAMP}/'\n",
    "os.mkdir(WORKING_DIRECTORY)\n",
    "\n",
    "import csv\n",
    "HP_FILEPATH = WORKING_DIRECTORY + 'q_hp.csv'\n",
    "with open(HP_FILEPATH, 'w', newline='') as f:\n",
    "    hp_file_columns = ['hp_config','time','val_acc-avg','val_acc-med','val_acc-std','train_acc-max','train_acc-min']\n",
    "    csv.writer(f).writerow(hp_file_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_string(config):\n",
    "    return str(config).translate(str.maketrans(',','-',\" '()\"))\n",
    "def get_history_filename(config):\n",
    "    return hp_string(config) + '-history.csv'\n",
    "\n",
    "get_history_filename((1, 32, 'cart_relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_hp_file(hp_config, df):\n",
    "    val_accuracy_tail = df['val_accuracy'][-TAIL_SIZE:]\n",
    "    accuracy_tail = df['accuracy'][-TAIL_SIZE:]\n",
    "    \n",
    "    with open(HP_FILEPATH, 'a', newline='') as f:\n",
    "        csv.writer(f).writerow([hp_string(hp_config),time.strftime(\"%H:%M:%S\", time.localtime()),np.average(val_accuracy_tail),np.median(val_accuracy_tail),np.std(val_accuracy_tail),np.max(accuracy_tail),np.min(accuracy_tail)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvnn.layers as complex_layers\n",
    "\n",
    "for hp in hp_list:\n",
    "    print('START', hp)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(complex_layers.ComplexInput(input_shape=(POINTS_NUM,)))\n",
    "    for layer_no in range(hp[param['hidden_layers']]):\n",
    "        model.add(complex_layers.ComplexDense(units=hp[param['neurons_num']], activation=hp[param['activation_func']]))\n",
    "    model.add(complex_layers.ComplexDense(units=CLASSES_NUM, activation='sigmoid_real'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(clipnorm=1.), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print('      model compiled')\n",
    "    print('      training...')\n",
    "    history = model.fit(points, q_labels, epochs=EPOCHS_NUM, validation_split=0.2, verbose=0, callbacks=[FreeMemory()])\n",
    "    print('      model trained')\n",
    "    df = make_dataframe(history)\n",
    "    append_hp_file(hp, df)\n",
    "    print('      hp_file appended')\n",
    "    df.to_csv(WORKING_DIRECTORY + get_history_filename(hp), index=False)\n",
    "    print('      history saved')\n",
    "    print('DONE ', hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cvnn.layers as complex_layers\n",
    "\n",
    "# model = tf.keras.models.Sequential()\n",
    "# model.add(complex_layers.ComplexInput(input_shape=(POINTS_NUM,)))\n",
    "# model.add(complex_layers.ComplexDense(units=64, activation='cart_relu'))\n",
    "# model.add(complex_layers.ComplexDense(units=CLASSES_NUM, activation='softmax_of_softmax_real_with_avg'))\n",
    "# model.compile(optimizer=tf.keras.optimizers.SGD(clipnorm=1.), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(points, q, epochs=250, validation_split=0.2, verbose=0, callbacks=[FreeMemory(10)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
